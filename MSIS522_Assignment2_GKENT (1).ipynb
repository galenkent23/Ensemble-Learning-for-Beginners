{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "MSIS522_Assignment2_GKENT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bYyOjdzhs1T",
        "colab_type": "text"
      },
      "source": [
        "# Decision Tree & Ensemble Learning\n",
        "\n",
        "Classification And Regression Trees (CART for short) is a term introduced by [Leo Breiman](https://en.wikipedia.org/wiki/Leo_Breiman) to refer to Decision Tree algorithms that can be used for classification or regression predictive modeling problems.\n",
        "\n",
        "In this lab assignment, you will implement various ways to calculate impurity which is used to split data in constructing the decision trees and apply the Decision Tree and ensemble learning algorithms to solve two real-world problems: a classification one and a regression one. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsJh8RbLhs1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import packages\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# make this notebook's output stable across runs\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0Wc52cZzgkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper functions used in this lab\n",
        "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n",
        "    \"\"\"\n",
        "    Plot the decision boundary of a learnt classifier\n",
        "    \"\"\"\n",
        "    x1s = np.linspace(axes[0], axes[1], 100)\n",
        "    x2s = np.linspace(axes[2], axes[3], 100)\n",
        "    x1, x2 = np.meshgrid(x1s, x2s)\n",
        "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
        "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
        "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
        "    if contour:\n",
        "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
        "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=1)\n",
        "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
        "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
        "    plt.axis(axes)\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
        "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wdJsuTqhs1a",
        "colab_type": "text"
      },
      "source": [
        "## Gini impurity and Entropy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzfYaZ-hhs1b",
        "colab_type": "text"
      },
      "source": [
        "#### Gini impurity\n",
        "\n",
        "The CART algorithm recursively splits the training set into two subsets using a single feature k and a threshold $t_k$. The best feature and threshold are chosen to produce the purest subsets weighted by their size. **Gini impurity** measures the impurity of the data points in a set and is used to evaluate how good a split is when the CART algorithm searches for the best pair of feature and the threshold.\n",
        "\n",
        "To compute Gini impurity for a set of items with J classes, suppose $i \\in \\{1, 2, \\dots, J\\}$ and let $p_i$ be the fraction of items labeled with class i in the set.\n",
        "\\begin{align}\n",
        "I(p) = 1 - \\sum_{i=1}^J p_i^2\n",
        "\\end{align}\n",
        "\n",
        "The following function calculates the gini impurity for a given set of data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNexBvnehs1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gini_impurity(x):\n",
        "    \"\"\"\n",
        "    This function calculate the Gini impurity for a given set of data points.\n",
        "\n",
        "    Args:\n",
        "    x: a numpy ndarray\n",
        "    \"\"\"\n",
        "    unique, counts = np.unique(x, return_counts=True)\n",
        "    probabilities = counts / sum(counts)\n",
        "    gini = 1 - sum([p*p for p in probabilities])\n",
        "\n",
        "    return gini"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmZaWVMxhs1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.testing.assert_equal(0, gini_impurity(np.array([1, 1, 1])))\n",
        "np.testing.assert_equal(0.5, gini_impurity(np.array([1, 0, 1, 0])))\n",
        "np.testing.assert_equal(3/4, gini_impurity(np.array(['a', 'b', 'c', 'd'])))\n",
        "np.testing.assert_almost_equal(2.0/3, gini_impurity(np.array([1, 2, 3, 1, 2, 3])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSsc9tL7hs1k",
        "colab_type": "text"
      },
      "source": [
        "#### Entropy\n",
        "\n",
        "Another popular measure of impurity is called **entropy**, which measures the average information content of a message. Entropy is zero when all messages are identical. When it applied to CART, a set's entropy is zero when it contains instances of only one class. Entropy is calculated as follows:\n",
        "\\begin{align}\n",
        "I(p) = - \\sum_{i=1}^J p_i log_2{p_i}\n",
        "\\end{align}\n",
        "\n",
        "<span style=\"color:orange\">**Question 1: In this exercise, you will implement the entropy function.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr8caP2Xhs1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entropy(x):\n",
        "    probs = [np.mean(x == c) for c in set(x)]\n",
        "    return np.sum(-p * np.log2(p) for p in probs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpOdGMWDhs1p",
        "colab_type": "code",
        "outputId": "62aed50d-1040-493c-eb72-499572b8b6d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "np.testing.assert_equal(0, entropy(np.array([1, 1, 1])))\n",
        "np.testing.assert_equal(1.0, entropy(np.array([1, 0, 1, 0])))\n",
        "np.testing.assert_equal(2.0, entropy(np.array(['a', 'b', 'c', 'd'])))\n",
        "np.testing.assert_almost_equal(1.58496, entropy(np.array([1, 2, 3, 1, 2, 3])), 4)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qX1XT12hs1t",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZGblkNkhs2g",
        "colab_type": "text"
      },
      "source": [
        "## Iris dataset\n",
        "\n",
        "The Iris data set contains the morphologic variation of Iris flowers of three related species (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each observation (see image below):\n",
        "- Sepal.Length: sepal length in centimeters.\n",
        "- Sepal.Width: sepal width in centimeters.\n",
        "- Petal.Length: petal length in centimeters.\n",
        "- Petal.Width: petal width in centimeters.\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg/180px-Kosaciec_szczecinkowaty_Iris_setosa.jpg\" style=\"width:250px\"></td>\n",
        "    <td><img src=\"https://www.math.umd.edu/~petersd/666/html/iris_with_labels.jpg\" width=\"250px\"></td>\n",
        "    <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/295px-Iris_virginica.jpg\" width=\"250px\"></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Iris setosa</td>\n",
        "    <td>Iris versicolor</td>\n",
        "    <td>Iris virginica</td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BfuNcWVhs2h",
        "colab_type": "code",
        "outputId": "71e2a0a6-adb4-46bf-ff2f-05110c9b995b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# load the iris train and test data from CSV files\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_train.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_test.csv')\n",
        "\n",
        "train_x = train.iloc[:,0:4]\n",
        "train_y = train.iloc[:,4]\n",
        "\n",
        "test_x = test.iloc[:,0:4]\n",
        "test_y = test.iloc[:,4]\n",
        "\n",
        "# print the number of instances in each class\n",
        "print(train_y.value_counts().sort_index())\n",
        "print(test_y.value_counts().sort_index())\n",
        "\n",
        "print(\"Number of train data: {}\".format(len(train_y)))\n",
        "print(\"Number of test data: {}\".format(len(test_y)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iris-setosa        34\n",
            "Iris-versicolor    32\n",
            "Iris-virginica     39\n",
            "Name: species, dtype: int64\n",
            "Iris-setosa        16\n",
            "Iris-versicolor    18\n",
            "Iris-virginica     11\n",
            "Name: species, dtype: int64\n",
            "Number of train data: 105\n",
            "Number of test data: 45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpe8jv7Qhs2f",
        "colab_type": "text"
      },
      "source": [
        "### Decision Tree Classifier\n",
        "\n",
        "<span style=\"color:orange\">**In this exercise, we will apply the Decision Tree classifier to classify the Iris flower data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JwQoOdPhs2j",
        "colab_type": "text"
      },
      "source": [
        "#### Train and visualize a simple Decision Tree\n",
        "\n",
        "<span style=\"color:orange\">**Question 2: create a decision tree with max_depth of 2.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9UVJlkZe8om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the iris train and test data from CSV files\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_train.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_test.csv')\n",
        "\n",
        "train_x = train.iloc[:,0:4]\n",
        "train_y = train.iloc[:,4]\n",
        "\n",
        "test_x = test.iloc[:,0:4]\n",
        "test_y = test.iloc[:,4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kQzTTFGlhs2k",
        "colab_type": "code",
        "outputId": "d161feb8-9a34-45c6-f433-032e1a0e07ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# TODO: read the scikit-learn doc on DecisionTreeClassifier and train a Decision Tree with max depth of 2\n",
        "\n",
        "parameters = {\n",
        "    \"max_depth\": [2], \n",
        "    #\"min_samples_split\": [0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc_grid = GridSearchCV(dtc, parameters, cv=3)\n",
        "dtc_grid.fit(train_x, train_y)\n",
        "\n",
        "# summarize the results of the grid search\n",
        "print(\"The best score is {}\".format(dtc_grid.best_score_))\n",
        "print(\"The best hyper parameter setting is {}\".format(dtc_grid.best_params_))\n",
        "\n",
        "# model initialization\n",
        "dt_model = DecisionTreeClassifier(max_depth=2)\n",
        "\n",
        "# train the model\n",
        "dt_model.fit(train_x, train_y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best score is 0.9428571428571427\n",
            "The best hyper parameter setting is {'max_depth': 2}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn25MVpchs2n",
        "colab_type": "text"
      },
      "source": [
        "Now let's visualize the decision tree we just trained on the iris dataset and see how it makes predictions. Note that if the following code does not work for you because the graphviz is missing, do not worry about it and you should still be able to move on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Y80Nilx1hs2o",
        "colab_type": "code",
        "outputId": "6bfdc1d9-0442-463c-9a09-4f226a3769a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from sklearn.externals.six import StringIO  \n",
        "from IPython.display import Image  \n",
        "from sklearn.tree import export_graphviz\n",
        "import pydotplus\n",
        "\n",
        "#dot_data = StringIO()\n",
        "#feature_names = train_x.columns\n",
        "#class_names = train_y.unique()\n",
        "#class_names.sort()\n",
        "#export_graphviz(dtc, out_file=dot_data, feature_names=feature_names, class_names=class_names, filled=True, rounded=True)\n",
        "#graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "#Image(graph.create_png())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgqeLw6Lhs2t",
        "colab_type": "text"
      },
      "source": [
        "Decision trees are easy to inteprete and is often referred to as *whitebox* machine learning algorithm. Let's see how this decision tree represented above makes predictions. Suppose you find an iris flower and want to classify it into setosa, versicolor or virginica. You start at the root node (the very top node in the tree). In this node, we check if the flower's patel length is smaller than or equal to 2.35 cm. If it is, we move to the left child and predict setosa to be its class. Otherwise, we move to the right child node. Then similarly we check if the petal length is smaller than or equal to 4.95 cm. If it is, we move to its left child node and predict versicolor to be its class. Otherwise, we move to its right child and predict virginica to be its class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W94pWMvIhs2t",
        "colab_type": "text"
      },
      "source": [
        "#### Prediction with Decision tree\n",
        "\n",
        "With this simple decision tree above, we can apply it to make predictions on the test dataset and evaluate its performance.\n",
        "\n",
        "<span style=\"color:orange\">**Question 3: make prediction using the trained decision tree model on the test data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugRPudSThs2u",
        "colab_type": "code",
        "outputId": "276e0563-4ec3-4e49-9bea-9d806494c7c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# TODO: use the trained decision tree model to make predictions on the test data and evaluate the model performance.\n",
        "\n",
        "train_z = dt_model.predict(train_x)\n",
        "train_z_prob = dt_model.predict_proba(train_x)[:,1]\n",
        "\n",
        "test_z = dt_model.predict(test_x)\n",
        "test_z_prob = dt_model.predict_proba(test_x)[:,1]\n",
        "\n",
        "print(\"model accuracy on train set: {}\".format(accuracy_score(train_y, train_z)))\n",
        "print(\"model confusion matrix (Train Set):\\n {}\".format(confusion_matrix(train_y, train_z, labels=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])))\n",
        "\n",
        "print(\"model accuracy on test set: {}\".format(accuracy_score(test_y, test_z)))\n",
        "print(\"model confusion matrix (Test Set):\\n {}\".format(confusion_matrix(test_y, test_z, labels=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model accuracy on train set: 0.9619047619047619\n",
            "model confusion matrix (Train Set):\n",
            " [[34  0  0]\n",
            " [ 0 31  1]\n",
            " [ 0  3 36]]\n",
            "model accuracy on test set: 0.9111111111111111\n",
            "model confusion matrix (Test Set):\n",
            " [[16  0  0]\n",
            " [ 0 17  1]\n",
            " [ 0  3  8]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMzcZjQQhs2x",
        "colab_type": "text"
      },
      "source": [
        "#### Hyper-parameters\n",
        "\n",
        "Hyper-parameter controls the complexity of the decision tree model. For example, the deeper the tree is, the more complex patterns the model will be able to capture. In this exercise, we train the decision trees with increasing number of maximum depth and plot its performance. We should see the accuracy of the training data increase as the tree grows deeper, but the accuracy on the test data might not as the model will eventually start to overfit and does not generalize well on the unseen test data.\n",
        "\n",
        "<span style=\"color:orange\">**Question 4: for each value of max_depth, we train a decision tree model and evaluate its accuracy on both train and test data, and plot both accuracies in the figure.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIWqwTGi2bi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the iris train and test data from CSV files\n",
        "train_dtc = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_train.csv')\n",
        "test_dtc = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_test.csv')\n",
        "\n",
        "train_x_dtc = train.iloc[:,0:4]\n",
        "train_y_dtc = train.iloc[:,4]\n",
        "\n",
        "test_x_dtc = test.iloc[:,0:4]\n",
        "test_y_dtc = test.iloc[:,4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw23r_MXhs2x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5a0bebac-1c2a-46b7-fd8d-5047624e690c"
      },
      "source": [
        "# TODO: train the decision tree model with various max_depth, make predictions and evaluate on both train and test data.\n",
        "\n",
        "parameters = {\n",
        "    \"max_depth\": [2, 3, 4, 5, 6] \n",
        "    # \"min_samples_split\": [0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc_grid = GridSearchCV(dtc, parameters, cv=3)\n",
        "dtc_grid.fit(train_x_dtc, train_y_dtc)\n",
        "\n",
        "# summarize the results of the grid search\n",
        "print(\"The best score is {}\".format(dtc_grid.best_score_))\n",
        "print(\"The best hyper parameter setting is {}\".format(dtc_grid.best_params_))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best score is 0.9619047619047619\n",
            "The best hyper parameter setting is {'max_depth': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ayau-R2x2CkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make prediction and evaluate the model performance on test data\n",
        "test_z_dtc = dtc_grid.predict(test_x_dtc)\n",
        "test_z_prob_dtc = dtc_grid.predict_proba(test_x_dtc)[:,1]\n",
        "\n",
        "train_z_dtc = dtc_grid.predict(train_x_dtc)\n",
        "train_z_prob_dtc = dtc_grid.predict_proba(train_x_dtc)[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L9Fglde2CSE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "ed19e9a9-7b3a-4bfb-d159-523c7b122e35"
      },
      "source": [
        "print(\"model accuracy on train set: {}\".format(accuracy_score(train_y_dtc, train_z_dtc)))\n",
        "print(\"model confusion matrix (Train Set):\\n {}\".format(confusion_matrix(train_y, train_z, labels=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])))\n",
        "\n",
        "print(\"model accuracy on test set: {}\".format(accuracy_score(test_y_dtc, test_z_dtc)))\n",
        "print(\"model confusion matrix (Test Set):\\n {}\".format(confusion_matrix(test_y, test_z, labels=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model accuracy on train set: 0.9809523809523809\n",
            "model confusion matrix (Train Set):\n",
            " [[34  0  0]\n",
            " [ 0 31  1]\n",
            " [ 0  3 36]]\n",
            "model accuracy on test set: 0.9777777777777777\n",
            "model confusion matrix (Test Set):\n",
            " [[16  0  0]\n",
            " [ 0 17  1]\n",
            " [ 0  3  8]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buYs3MwBxfCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the decision boundary for decision tree classifier\n",
        "#plot_decision_boundary(dtc_grid, train_x_dtc.values, train_y_dtc.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iaLr1ufhs20",
        "colab_type": "text"
      },
      "source": [
        "#### Fine-tune the decision tree classifier\n",
        "\n",
        "Decision tree is a very powerful model with very few assumptions about the incoming training data (unlike linear models, which assume the data linear), however, it is more likely to overfit the data and won't generalize well to unseen data. To void overfitting, we need to restrict the decision tree's freedom during training via regularization (e.g. max_depth, min_sample_split, max_leaf_nodes and etc.).\n",
        "\n",
        "To fine-tune the model and combat overfitting, use grid search with cross-validation (with the help of the GridSearchCV class) to find the best hyper-parameter settings for the DecisionTreeClassifier. In particular, we would like to fine-tune the following hyper-parameters:\n",
        "- **criteria**: this defines how we measure the quality of a split. we can choose either \"gini\" for the Gini impurity or \"entropy\" for the information gain.\n",
        "- **max_depth**: the maximum depth of the tree. This indicates how deep the tree can be. The deeper the tree, the more splits it has and it captures more information about the data. But meanwhile, deeper trees are more likely to overfit the data. For this practice, we will choose from {1, 2, 3} given there are only 4 features in the iris dataset.\n",
        "- **min_samples_split**: This value represents the minimum number of samples required to split an internal node. The smaller this value is, the deeper the tree will grow, thus more likely to overfit. On the other hand, if the value is really large (the size of the training data in the extreme case), the tree will be very shallow and could suffer from underfit. In this practice, we choose from {0.01, 0.05, 0.1, 0.2}.\n",
        "\n",
        "<span style=\"color:orange\">**Question 5: Use grid search with 3-fold cross-validation to fine-tune the decision tree model and output the best hyper-parameters.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54nxQ_3N5WJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the iris train and test data from CSV files\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_train.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_test.csv')\n",
        "\n",
        "train_x = train.iloc[:,0:4]\n",
        "train_y = train.iloc[:,4]\n",
        "\n",
        "test_x = test.iloc[:,0:4]\n",
        "test_y = test.iloc[:,4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fuI5TWkhs21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6668a670-d0aa-4c75-c99e-6b24c94f4bf4"
      },
      "source": [
        "# TODO: fine-tune the model, use grid search with 3-fold cross-validation.\n",
        "parameters = {\n",
        "    \"max_depth\": [2, 3, 4, 5, 6], \n",
        "    \"min_samples_split\": [0.05, 0.1, 0.2]\n",
        "    #\"criteria\": []\n",
        "}\n",
        "\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc_grid = GridSearchCV(dtc, parameters, cv=3)\n",
        "dtc_grid.fit(train_x, train_y)\n",
        "\n",
        "# summarize the results of the grid search\n",
        "print(\"The best score is {}\".format(dtc_grid.best_score_))\n",
        "print(\"The best hyper parameter setting is {}\".format(dtc_grid.best_params_))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best score is 0.9619047619047619\n",
            "The best hyper parameter setting is {'max_depth': 3, 'min_samples_split': 0.05}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXeHA4TThs24",
        "colab_type": "text"
      },
      "source": [
        "#### Prediction and Evaluation\n",
        "\n",
        "Now we have a fine-tuned decision tree classifier based on the training data, let's apply this model to make predictions on the test data and evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzGEUWFPhs24",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "385e0c82-0591-4d28-f078-ac92745098c8"
      },
      "source": [
        "dtc_grid.predict(test_x)\n",
        "\n",
        "print(\"model accuracy: {}\".format(accuracy_score(test_y, test_z)))\n",
        "print(\"model confusion matrix:\\n {}\".format(confusion_matrix(test_y, test_z, labels=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model accuracy: 0.9111111111111111\n",
            "model confusion matrix:\n",
            " [[16  0  0]\n",
            " [ 0 17  1]\n",
            " [ 0  3  8]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLrLCXpbIHI2",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest\n",
        "\n",
        "**Question 6: Apply Random Forest together with Gridsearch to the Iris dataset and evaluate its accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTzDdKnoIDM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TODO\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfaAOM9gYyMy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "7623aebb-85fa-4871-a5f9-2fbbcad10612"
      },
      "source": [
        "# load the iris train and test data from CSV files\n",
        "train_rf = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_train.csv')\n",
        "test_rf = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_test.csv')\n",
        "\n",
        "train_x_rf = train_rf.iloc[:,0:4]\n",
        "train_y_rf = train_rf.iloc[:,4]\n",
        "\n",
        "test_x_rf = test_rf.iloc[:,0:4]\n",
        "test_y_rf = test_rf.iloc[:,4]\n",
        "\n",
        "# print the number of instances in each class\n",
        "print(train_y_rf.value_counts().sort_index())\n",
        "print(test_y_rf.value_counts().sort_index())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iris-setosa        34\n",
            "Iris-versicolor    32\n",
            "Iris-virginica     39\n",
            "Name: species, dtype: int64\n",
            "Iris-setosa        16\n",
            "Iris-versicolor    18\n",
            "Iris-virginica     11\n",
            "Name: species, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4JBC5qqYUT_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6bde29c3-74f7-40a6-b446-467e1f6f0b9e"
      },
      "source": [
        "parameters = {\n",
        "    \"n_estimators\": [20, 40],\n",
        "    \"max_depth\": [2, 4], \n",
        "    \"min_samples_split\": [0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "rfc_grid = GridSearchCV(RandomForestClassifier(n_jobs=-1, random_state=0), parameters, cv=3)\n",
        "rfc_grid.fit(train_x_rf, train_y_rf)\n",
        "\n",
        "# summarize the results of the grid search\n",
        "print(\"The best score is {}\".format(rfc_grid.best_score_))\n",
        "print(\"The best hyper parameter setting is {}\".format(rfc_grid.best_params_))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best score is 0.9619047619047619\n",
            "The best hyper parameter setting is {'max_depth': 2, 'min_samples_split': 0.05, 'n_estimators': 40}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUykwadIYdTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "524f2187-205c-416b-d364-2fba71357d40"
      },
      "source": [
        "#make prediction and evaluate the model performance on test data\n",
        "test_z_rf = rfc_grid.predict(test_x_rf)\n",
        "test_z_prob_rf = rfc_grid.predict_proba(test_x_rf)[:,1]\n",
        "\n",
        "print(\"model accuracy: {}\".format(accuracy_score(test_y_rf, test_z_rf)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model accuracy: 0.9555555555555556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKgNC7kzIWNC",
        "colab_type": "text"
      },
      "source": [
        "### Adaboost\n",
        "\n",
        "**Question 7: Apply Adaboost together with Gridsearch to the Iris dataset and evaluate its accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pVjKdqNaSDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4N02CqhZ5fz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "93a26a53-4852-4937-ba83-1e721ea756f8"
      },
      "source": [
        "# load the iris train and test data from CSV files\n",
        "train_ada = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_train.csv')\n",
        "test_ada = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_test.csv')\n",
        "\n",
        "train_x_ada = train_ada.iloc[:,0:4]\n",
        "train_y_ada = train_ada.iloc[:,4]\n",
        "\n",
        "test_x_ada = test_ada.iloc[:,0:4]\n",
        "test_y_ada = test_ada.iloc[:,4]\n",
        "\n",
        "# print the number of instances in each class\n",
        "print(train_y_ada.value_counts().sort_index())\n",
        "print(test_y_ada.value_counts().sort_index())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iris-setosa        34\n",
            "Iris-versicolor    32\n",
            "Iris-virginica     39\n",
            "Name: species, dtype: int64\n",
            "Iris-setosa        16\n",
            "Iris-versicolor    18\n",
            "Iris-virginica     11\n",
            "Name: species, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDcP-xy_IXFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "73cda7c2-84e2-496b-b86e-e50ee44b1b99"
      },
      "source": [
        "### TODO ADA BOOST\n",
        "parameters = {\n",
        "    \"n_estimators\": [20, 40],\n",
        "    \"learning_rate\": [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=4), random_state=0)\n",
        "adaboost_grid = GridSearchCV(adaboost, parameters, cv=3)\n",
        "adaboost_grid.fit(train_x_ada, train_y_ada)\n",
        "\n",
        "# summarize the results of the grid search\n",
        "print(\"The best score is {}\".format(adaboost_grid.best_score_))\n",
        "print(\"The best hyper parameter setting is {}\".format(adaboost_grid.best_params_))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best score is 0.9523809523809522\n",
            "The best hyper parameter setting is {'learning_rate': 0.01, 'n_estimators': 20}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AoyWQ8S8JLX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85a5ba94-0958-4b9e-8085-b4a761cd802b"
      },
      "source": [
        "# TODO: make prediction and evaluate the model performance on test data\n",
        "test_z_ada = adaboost_grid.predict(test_x_ada)\n",
        "test_z_prob_ada = adaboost_grid.predict_proba(test_x_ada)[:,1]\n",
        "\n",
        "print(\"model accuracy: {}\".format(accuracy_score(test_y_ada, test_z_ada)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model accuracy: 0.9777777777777777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpZFN60NIWdK",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Boosting\n",
        "\n",
        "**Question 8: Apply Boosting together with Gridsearch to the Iris dataset and evaluate its accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51oi8-18advS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import GradientBoostingRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXWT7AiMadzD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "256eb4eb-40fb-4055-c530-e6830f9f30dc"
      },
      "source": [
        "# load the iris train and test data from CSV files\n",
        "train_gb = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_train.csv')\n",
        "test_gb = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/iris_test.csv')\n",
        "\n",
        "train_x_gb = train_gb.iloc[:,0:4]\n",
        "train_y_gb = train_gb.iloc[:,4]\n",
        "\n",
        "test_x_gb = test_gb.iloc[:,0:4]\n",
        "test_y_gb = test_gb.iloc[:,4]\n",
        "\n",
        "# print the number of instances in each class\n",
        "print(train_y_gb.value_counts().sort_index())\n",
        "print(test_y_gb.value_counts().sort_index())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iris-setosa        34\n",
            "Iris-versicolor    32\n",
            "Iris-virginica     39\n",
            "Name: species, dtype: int64\n",
            "Iris-setosa        16\n",
            "Iris-versicolor    18\n",
            "Iris-virginica     11\n",
            "Name: species, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmRcvEkSIXrR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "420d326c-ef6b-4456-b010-d2d9dbc6d50b"
      },
      "source": [
        "#fine-tune Gradient Boosted Trees using grid search with cross-validation (GridSearchCV).\n",
        "parameters = {\n",
        "    \"loss\":[\"deviance\"],\n",
        "    \"learning_rate\": [0.01, 0.1],\n",
        "    \"min_samples_split\": [0.05, 0.1, 0.2],\n",
        "    \"max_depth\":[2, 4],\n",
        "    \"n_estimators\":[100]\n",
        "}\n",
        "\n",
        "gbc_grid = GridSearchCV(GradientBoostingClassifier(), parameters, cv=3, n_jobs=-1)\n",
        "gbc_grid.fit(train_x_gb, train_y_gb)\n",
        "\n",
        "# summarize the results of the grid search\n",
        "print(\"The best score is {}\".format(gbc_grid.best_score_))\n",
        "print(\"The best hyper parameter setting is {}\".format(gbc_grid.best_params_))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best score is 0.9619047619047619\n",
            "The best hyper parameter setting is {'learning_rate': 0.01, 'loss': 'deviance', 'max_depth': 2, 'min_samples_split': 0.1, 'n_estimators': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTlPSt2Uhs26",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFuRnosu8qSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bccc01d0-e347-4bd7-bb01-a59d9367f0c7"
      },
      "source": [
        "#make prediction and evaluate the model performance on test data\n",
        "test_z_gb = gbc_grid.predict(test_x_gb)\n",
        "test_z_prob_gb = gbc_grid.predict_proba(test_x_gb)[:,1]\n",
        "\n",
        "print(\"model accuracy: {}\".format(accuracy_score(test_y_gb, test_z_gb)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model accuracy: 0.9777777777777777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oyoZxHnxHknE"
      },
      "source": [
        "**BONUS POINT: we will apply the supervised learning models we learnt so far to predict the California housing prices.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3odrweXdhs28",
        "colab_type": "text"
      },
      "source": [
        "## California Housing Dataset\n",
        "\n",
        "The California Housing dataset appeared in a 1997 paper titled Sparse Spatial Autoregressions by Pace, R. Kelley and Ronald Barry, published in the Statistics and Probability Letters journal. They built it using the 1990 California census data. It contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zXK2RFshs29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load train and test data from CSV files.\n",
        "train_ca = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/housing_train.csv')\n",
        "test_ca = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/housing_test.csv')\n",
        "\n",
        "train_x_ca = train_ca.iloc[:,0:8]\n",
        "train_y_ca = train_ca.iloc[:,8]\n",
        "\n",
        "test_x_ca = test_ca.iloc[:,0:8]\n",
        "test_y_ca = test_ca.iloc[:,8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd6SKl0tipr2",
        "colab_type": "text"
      },
      "source": [
        "**Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H__-LNkBi9xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load train and test data from CSV files.\n",
        "train_dt = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/housing_train.csv')\n",
        "test_dt = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/housing_test.csv')\n",
        "\n",
        "train_x_dt = train_dt.iloc[:,0:8]\n",
        "train_y_dt = train_dt.iloc[:,8]\n",
        "\n",
        "test_x_dt = test_dt.iloc[:,0:8]\n",
        "test_y_dt = test_dt.iloc[:,8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL-zgcBxfLiA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "f9b17479-3f03-4f5e-a6ba-1f6792d1f1c1"
      },
      "source": [
        "# Build Decision Tree classifier\n",
        "parameters = {\n",
        "    \"max_depth\": [2, 4, 6 , 8], \n",
        "    \"min_samples_split\": [0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc_grid = GridSearchCV(dtc, parameters, cv=3)\n",
        "dtc_grid.fit(train_x_dt, train_y_dt)\n",
        "\n",
        "# summarize the results of the grid search\n",
        "print(\"The best Decision Tree score is {}\".format(dtc_grid.best_score_))\n",
        "print(\"The best Decision Tree hyper parameter setting is {}\".format(dtc_grid.best_params_))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best Decision Tree score is 0.04900332225913621\n",
            "The best Decision Tree hyper parameter setting is {'max_depth': 6, 'min_samples_split': 0.05}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBSvatBrfLwP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0867c5f-2dc6-4386-dc2f-9a13d88c1451"
      },
      "source": [
        "# make prediction and evaluate the model performance on test data\n",
        "test_z_dt = dtc_grid.predict(test_x_dt)\n",
        "test_z_prob_dt = dtc_grid.predict_proba(test_x_dt)[:,1]\n",
        "\n",
        "print(\"Decision Tree model accuracy: {}\".format(accuracy_score(test_y_dt, test_z_dt)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decision Tree model accuracy: 0.04861111111111111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_d-E4UNiw9C",
        "colab_type": "text"
      },
      "source": [
        "**Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt76xWyOjeZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load train and test data from CSV files.\n",
        "train_rf = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/housing_train.csv')\n",
        "test_rf = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/housing_test.csv')\n",
        "\n",
        "train_x_rf = train_rf.iloc[:,0:8]\n",
        "train_y_rf = train_rf.iloc[:,8]\n",
        "\n",
        "test_x_rf = test_rf.iloc[:,0:8]\n",
        "test_y_rf = test_rf.iloc[:,8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd2UG_6RfMDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "9f09b0ff-3e5a-4b28-cd00-03eae1017e8b"
      },
      "source": [
        "# TODO: fine-tune Random Forest classifier using grid search with cross-validation (GridSearchCV).\n",
        "parameters = {\n",
        "    \"n_estimators\": [20, 40],\n",
        "    \"max_depth\": [2, 4], \n",
        "    \"min_samples_split\": [0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "rfc_grid = GridSearchCV(RandomForestClassifier(n_jobs=-1, random_state=0), parameters, cv=3)\n",
        "rfc_grid.fit(train_x_rf, train_y_rf)\n",
        "\n",
        "# summarize the results of the grid search\n",
        "print(\"The best Random Forest score is {}\".format(rfc_grid.best_score_))\n",
        "print(\"The best Random Foresthyper parameter setting is {}\".format(rfc_grid.best_params_))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best Random Forest score is 0.04741140642303434\n",
            "The best Random Foresthyper parameter setting is {'max_depth': 4, 'min_samples_split': 0.05, 'n_estimators': 20}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBgfsl2ZfMMk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94e38d50-7e35-47d2-d452-4658c26c85e6"
      },
      "source": [
        "# TODO: make prediction and evaluate the model performance on test data\n",
        "test_z_rf = rfc_grid.predict(test_x_rf)\n",
        "test_z_prob_rf = rfc_grid.predict_proba(test_x_rf)[:,1]\n",
        "\n",
        "print(\"Random Forest model accuracy: {}\".format(accuracy_score(test_y_rf, test_z_rf)))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest model accuracy: 0.04699612403100775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw05jexMjq1x",
        "colab_type": "text"
      },
      "source": [
        "**ADABOOST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDku5CIApVwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKQrhkMrjtoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load train and test data from CSV files.\n",
        "train_ada = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/housing_train.csv')\n",
        "test_ada = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/housing_test.csv')\n",
        "\n",
        "train_x_ada = train_ada.iloc[:,0:8]\n",
        "train_y_ada = train_ada.iloc[:,8]\n",
        "\n",
        "test_x_ada = test_ada.iloc[:,0:8]\n",
        "test_y_ada = test_ada.iloc[:,8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WG8trCLfMIZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "75ed3b59-0b28-46d4-cd76-6b6922e6f5d2"
      },
      "source": [
        "# TODO: fine-tune Adaboost with decision tree (max_depth=4) as base learners using grid search with cross-validation (GridSearchCV).\n",
        "parameters = {\n",
        "    \"n_estimators\": [20, 40],\n",
        "    \"learning_rate\": [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=4), random_state=0)\n",
        "adaboost_grid = GridSearchCV(adaboost, parameters, cv=3)\n",
        "adaboost_grid.fit(train_x_ada, train_y_ada)\n",
        "\n",
        "# summarize the results of the grid search\n",
        "print(\"The best AdaBoost score is {}\".format(adaboost_grid.best_score_))\n",
        "print(\"The best AdaBoost hyper parameter setting is {}\".format(adaboost_grid.best_params_))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best AdaBoost score is 0.04934939091915836\n",
            "The best AdaBoost hyper parameter setting is {'learning_rate': 0.01, 'n_estimators': 40}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mStiBFjFfMCA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45be0217-9743-40e7-e8c4-407ec3edca16"
      },
      "source": [
        "# TODO: make prediction and evaluate the model performance on test data\n",
        "test_z_ada = adaboost_grid.predict(test_x_ada)\n",
        "test_z_prob_ada = adaboost_grid.predict_proba(test_x_ada)[:,1]\n",
        "\n",
        "print(\"model accuracy: {}\".format(accuracy_score(test_y_ada, test_z_ada)))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model accuracy: 0.04941860465116279\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiHA4v-uYatf",
        "colab_type": "text"
      },
      "source": [
        "**Gradient Boost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1QdVqAssF5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load train and test data from CSV files.\n",
        "train_gb = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/housing_train.csv')\n",
        "test_gb = pd.read_csv('https://raw.githubusercontent.com/zariable/data/master/housing_test.csv')\n",
        "\n",
        "train_x_gb = train_gb.iloc[:,0:8]\n",
        "train_y_gb = train_gb.iloc[:,8]\n",
        "\n",
        "test_x_gb = test_gb.iloc[:,0:8]\n",
        "test_y_gb = test_gb.iloc[:,8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBhvmBDzsF1O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b6824ee1-05d0-40c6-ffef-ef149b005b77"
      },
      "source": [
        "# TODO: fine-tune Gradient Boosted Trees using grid search with cross-validation (GridSearchCV).\n",
        "parameters = {\n",
        "    \"loss\":[\"deviance\"],\n",
        "    \"learning_rate\": [0.01, 0.1],\n",
        "    \"min_samples_split\": [0.05, 0.1, 0.2],\n",
        "    \"max_depth\":[2, 4],\n",
        "    \"n_estimators\":[100]\n",
        "}\n",
        "\n",
        "gbc_grid = GridSearchCV(GradientBoostingClassifier(), parameters, cv=3, n_jobs=-1)\n",
        "gbc_grid.fit(train_x_gb, train_y_gb)\n",
        "\n",
        "# summarize the results of the grid search\n",
        "print(\"The best Gradient Boost score is {}\".format(gbc_grid.best_score_))\n",
        "print(\"The best Gradient Boost hyper parameter setting is {}\".format(gbc_grid.best_params_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWYIMwRGsFxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: make prediction and evaluate the model performance on test data\n",
        "test_z_gb = gbc_grid.predict(test_x_gb)\n",
        "test_z_prob_gb = gbc_grid.predict_proba(test_x_gb)[:,1]\n",
        "\n",
        "print(\"Gradient Boost model accuracy: {}\".format(accuracy_score(test_y_gb, test_z_gb)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zuicq9ez9TEf",
        "colab_type": "text"
      },
      "source": [
        "# Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE67NQQFfL6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decision Tree Results\n",
        "print(\"The best Decision Tree score is {}\".format(dtc_grid.best_score_))\n",
        "print(\"The best Decision Tree hyper parameter setting is {}\".format(dtc_grid.best_params_))\n",
        "print(\"Decision Tree model accuracy: {}\".format(accuracy_score(test_y_dt, test_z_dt)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvCEAfI3n1yW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random Forest Results\n",
        "print(\"The best Random Forest score is {}\".format(rfc_grid.best_score_))\n",
        "print(\"The best Random Foresthyper parameter setting is {}\".format(rfc_grid.best_params_))\n",
        "print(\"Random Forest model accuracy: {}\".format(accuracy_score(test_y_rf, test_z_rf)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUJOvnxrn18d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AdaBoost Results\n",
        "print(\"The best AdaBoost score is {}\".format(adaboost_grid.best_score_))\n",
        "print(\"The best AdaBoost hyper parameter setting is {}\".format(adaboost_grid.best_params_))\n",
        "print(\"model accuracy: {}\".format(accuracy_score(test_y_ada, test_z_ada)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XkBd0rvplNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gradient Boost Results\n",
        "print(\"The best Gradient Boost score is {}\".format(gbc_grid.best_score_))\n",
        "print(\"The best Gradient Boost hyper parameter setting is {}\".format(gbc_grid.best_params_))\n",
        "print(\"Gradient Boost model accuracy: {}\".format(accuracy_score(test_y_gb, test_z_gb)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-zitGTxhs3E",
        "colab_type": "text"
      },
      "source": [
        "### End of Assignment 2\n",
        "---\n"
      ]
    }
  ]
}